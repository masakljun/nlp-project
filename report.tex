%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{tcolorbox}
\usepackage{calc}
\usepackage{booktabs}


\newsavebox\mybox
\newenvironment{aquote}[1]
  {\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
  {\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}

\graphicspath{{fig/}}


\newtcbox{\labelbox}[1][red]{on line,
arc=0pt,outer arc=0pt,colback=#1!10!white,colframe=#1!50!black,
boxsep=0pt,left=1pt,right=1pt,top=1pt,bottom=0.8pt,
boxrule=0pt,bottomrule=0.6pt,toprule=0.6pt,width=5cm}

\newcommand\bm{0.2pt}


%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2021}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Offensive language exploratory analysis} 

% Authors (student competitors) and their info
\Authors{Maša Kljun, Matija Teršek}

% Advisors
\affiliation{\textit{Advisors: Slavko Žitnik}}

% Keywords
\Keywords{Hate speech, TF-IDF, embeddings, exploratory analysis, NLP ...}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
In this paper we focus on the exploratory analysis of 10 different subgroups of hate speech. We use natural language processing techniques in order to find the underlying structure and connections/relations between the subgroups. We focus on data extracted from Twitter and online forums. First we use classic approaches, such as TF-IDF, BoW, and LDA, then we move on to more sophisticated methods such as embeddings. \textcolor{blue}{Note to professor: More to come in the next submission.}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
In the last few years social media grew exponentially and with it also the ability of people to express themselves online. By enabling people to write on different online platforms without even identifying themselves it lead to a new era of freedom of speech. As this new medium for communication and writing brought many positive things, it also has its downside. Social media has become a place where heated discussions happen and often result in insults and hatred. It is an important task to recognize hate speech and to prevent it.

Hate speech is defined as \textit{abusive or threatening speech or writing that expresses prejudice against a particular group, especially on the basis of race, religion, or sexual orientation}\cite{hate_speech}. We can see that the definiton is very vague. Having said that, the goal of our paper is to help distinguish different types of hate speech and find the specific keywords of its subgroups in order to explain its structure. This could help with its identification and classification. In this paper we focus on ten subgroups of hate speech - \textit{abusive, hateful, spam, general hate speech, profane, offensive, cyberbullying, racism, sexism, } and \textit{benevolent sexism}. With understanding the structure of these groups, the goal is to also find similarities and connections between them.

There has been done a lot of research regarding the hate speech, however these works are usually focused on the classification of hate speech. One of the first works include \cite{spertus1997smokey} who built the decision tree based classifier Smokey for abusive message recogniton and classification. Some other works that focus mainly on classification include \cite{waseem2016you} who compare the classification accuracy of models trained on expert and amateur annotations, \cite{gamback2017using} who use convolutional neural networks for classification into four predefined categories, and \cite{martins2018hate} who use different natural language processing techniques for expanding datasets with emotional information for better classification. In the last years, especially deep learning models are often used for detection and classification of hate speech, such as \cite{rizoiu2019transfer} who propose a sophisticated method that is a combination of a deep neural network architecture with transfer learning.
There is a also a lot of related work that focuses on creating large datasets such as \cite{chung2019conan} who create a large-scale, multilingual, expert based dataset of hate speech. 

What is less common in the research area of hate speech is analysis of relationships between different types of hate speech and the importance of specific keywords. Some examples include \cite{xu2012learning}, who try to separate bullying from other social media posts and try to discover topic of bullying using topic modeling with Latent Dirichlet Allocation (LDA). \cite{calderon2020topic} model hate speech against immigrants on Twitter in Spain. They try to find underlying topic of hate speech using LDA, discovering features of different dimensions of hatespeech, including foul language, humiliation, irony, etc. \cite{schmidt2017survey} conduct a survey about hate speech detection and describe key areas that have been explored, regarding the topic modeling, as well as sentiment analysis.

\textcolor{blue}{Note to professor: Our goal for the future is first a deeper analysis of the obtained data. We will try to extract some underlying topics for each label and present the main findings (LDA). Use TF-IDF to find the most common $n$-grams for each label. And in the future focus on the newer approaches, such as embedding with the further goal of exploring the relationships between types of hate speech. We hope that we can determine a more precise plan on the first submission defense.}

\textcolor{gray}{This paper is organized as follows: TODO at the end of the report}
%------------------------------------------------

\section*{Data}

We use six publicly available datasets for our exploratory analysis. We combine datasets \cite{waseem2016you}, \cite{rizoiu2019transfer}, and \cite{jha2017does} into one large dataset (reffered to as Dataset SRB) as they include same categories of hate speech. We make labels \textit{sexism}, \textit{racism}, and \textit{both} from \cite{waseem2016you} and \cite{rizoiu2019transfer}. The third dataset (\cite{jha2017does}) that we use contains label \textit{hostile sexism}, where marked tweets are already included in the first two datasets under \textit{sexism}, and label \textit{benevolent sexism}, which we rename to \textit{benevolent}. We obtain a dataset with $6069$ samples that are labeled either $sexism$, $racism$, $both$, or $benevolent$.

The fourth dataset (reffered to as Dataset AHS)\cite{founta2018large} that we use has $3$ categories - \textit{abusive,
hateful, spam}. As this is the original dataset no additional merging is needed. We obtain a dataset with $13776$ tweets with the mentioned labels. Note that we exclude \textit{None} label from both datasets, as we do not need it for the analysis.

We show the distribution of individual categories from datasets SRB and AHS in Figures \ref{fig:distribution_tweets_dataset1} and \ref{fig:distribution_tweets_dataset2}, respectively. Note that the numbers of samples might not match the numbers in the original papers, due to the Twitter removing the tweets, making them unavailable for us to analyze.

Additionally we use the dataset of comments extracted from the League of Legends community \cite{bretschneider2016detecting}. We preprocess the dataset given in the SQL format to a more readable CSV form and keep only the posts that are annotated as harassment. We obtain $259$ examples of cyberbullying examples.

The last dataset that we use was designed for the problem of the hate speech identification and classification, but we use the labels from the train and test set and merge them into one big dataset that we use for our analysis. It provides tags of \textit{hatespeech, profane, and offensive}, so we refer to the dataset as HPO. It consists of 2549 tweets, distribution of which can be seen in Figure \ref{fig:distribution_tweets_hpo}. For each subgroup of hate speech, we provide an example from the datasets.

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Racism}} - "He can't be a server at our restaurant, that beard makes him look like a terrorist." Everyone laughs. \#fuckthanksgiving
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Sexism}} - \#katieandnikki stop calling yourselves pretty and hot..you're not and saying it a million times doesn't make you either...STFU
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Benevolent}} - It's "NEXT to every successful man, there's a woman"
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Spam}} - RT @OnlyLookAtMino: [!!] \#WINNER trending \#1 on melon search
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Abusive}} - You Worried About Somebody Bein Ugly... Bitch You Ugly...
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Hateful}} - i hope leaders just kick retards that fake leave teams today
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Cyberbullying}} - plot twist she's a fggt
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Hatespeech}} - Johnson you liar. You don't give a flying one for the Irish
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Offensive}} - \#FuckTrump And retired porn star Melania too.
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Profane}} - Fuck Trump and anybody who voted for that Lyin POS!  \#FuckTrump
\end{tcolorbox}


\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{distribution_tweets_dataset1.png}
	\caption{\textbf{Distribution of tweets in SRB dataset.} This figure shows the distribution of hate speech categories in the SRB dataset. We can see that \textit{sexism} and \textit{benevolent} are well represented, whereas \textit{racism} and \textit{both} are far less frequent. Original set contains more tweets labeled \textit{racism}, but due to their removal we cannot obtain them.}
	\label{fig:distribution_tweets_dataset1}
\end{figure}

\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{distribution_tweets_dataset2.png}
	\caption{\textbf{Distribution of tweets in AHS dataset.} We see that the \textit{spam} is the most represented label in the dataset, which represents the majority of the dataset. This is followed by the \textit{abusive} tweets and there is the least \textit{hateful} tweets. We can see that categories in this dataset are well represented.}
	\label{fig:distribution_tweets_dataset2}
\end{figure}

\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{distribution_tweets_hpo.png}
	\caption{\textbf{Distribution of tweets in HPO dataset.} The most used label is \textit{hatespeech}. It is followed by \textit{profane} and then \textit{offensive}, which have a similar number of tweets.}
	\label{fig:distribution_tweets_hpo}
\end{figure}

\section*{Data preprocessing}
Before applying any methods we first preprocess all of our data. We separate datasets into subgroups only, where each contains multiple documents - tweets belonging to this category. We remove retweet text RT, hyperlinks, hashtags, taggings, new lines, and zero length tweets. We further filter out tokens that not contain letters, e.g., raw punctuation.

\section*{Methodology}
\subsection*{TF-IDF}
We start off with a traditional method TF-IDF as we want to see the most relevant words for each category of offensive language. We show the results in Table \ref{tab:tf-idf}

\begin{table}[htb]
\centering
\begin{tabular}{l|l}
\toprule
\textbf{category}   & \textbf{unigrams with highest tf-idf score} \\ \midrule
racism     & white, terror, coon                \\ \hline
sexism     & hot, hard, feminazi                \\ \hline
benevolent & nasty, sassy, classy               \\ \hline
abusive    & accomplish, ho, outrag, fake, act  \\ \hline
hateful    & religi, discrimin, freedom, lgbt, claim    \\ \hline
spam       & game, laptop, giveaway             \\ \hline
cyberbullying       & forev, surpris, liter, fake, ez             \\ \hline
hatespeech      & jai, aliv, chant, ram, claim             \\ \hline
benevolent       & women, sassi, nasti, classi, gonna           \\ \hline
identity hate       & fuck, dad, shit, said, smoke             \\ \hline
insult       & hore, delet, close, go, fun        \\ \hline
obscene       & go, like, fuck, wikipedia, shit             \\ \hline
offensive       & peac, journalist, agenda, danger, hypocrit            \\ \hline
profane       & friend, zomato, flipthesen, fire, first             \\ \hline
profane       & TODO             \\ \hline
offensive       & TODO             \\ \bottomrule
\end{tabular}
\caption{bb}
\label{tab:tf-idf}
\end{table}
%------------------------------------------------

\section*{Discussion}

Use the Discussion section to objectively evaluate your work, do not just put praise on everything you did, be critical and exposes flaws and weaknesses of your solution. You can also explain what you would do differently if you would be able to start again and what upgrades could be done on the project in the future.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}