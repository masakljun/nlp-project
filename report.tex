%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{tcolorbox}
\usepackage{calc}


\newsavebox\mybox
\newenvironment{aquote}[1]
  {\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
  {\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}

\graphicspath{{fig/}}


\newtcbox{\labelbox}[1][red]{on line,
arc=0pt,outer arc=0pt,colback=#1!10!white,colframe=#1!50!black,
boxsep=0pt,left=1pt,right=1pt,top=1pt,bottom=0.8pt,
boxrule=0pt,bottomrule=0.6pt,toprule=0.6pt,width=5cm}


%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2021}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Offensive language exploratory analysis} 

% Authors (student competitors) and their info
\Authors{Maša Kljun, Matija Teršek}

% Advisors
\affiliation{\textit{Advisors: Slavko Žitnik}}

% Keywords
\Keywords{Keyword1, Keyword2, Keyword3 ...}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
asdasdasd
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
In the last few years social media grew exponentially and with it also the ability of people to express themselves online. By enabling people to write on different online platforms without even identifying themselves it lead to a new era of freedom of speech. As this new medium for communication and writing brought many positive things, it also has its downside. Social media has become a place where heated discussions happen and often result in insults and hatred. It is an important task to recognize hate speech and to prevent it.

Hate speech is defined as \textit{abusive or threatening speech or writing that expresses prejudice against a particular group, especially on the basis of race, religion, or sexual orientation.}\cite{hate_speech}. However, we can see that the definiton is very vague. Having said that, the goal of our paper is to help distinguish different types of hate speech and find the specific keywords of its subgroups in order to explain its structure. This could help with its identification and classification. 

There has been done a lot of research regarding the hate speech, however these works are usually focused on the classification of hate speech. One of the first works include \cite{spertus1997smokey} who built the decision tree based classifier Smokey for abusive message recogniton and classification. Some other works that focus mainly on classification include \cite{waseem2016you} who compare the classification accuracy of models trained on expert and amateur annotations, \cite{gamback2017using} who use convolutional neural networks for classification into four predefined categories, and \cite{martins2018hate} who use different natural language processing techniques for expanding datasets with emotional information for better classification. In the last years, especially deep learning models are often used for detection and classification of hate speech, such as \cite{rizoiu2019transfer} who propose a sophisticated method that is a combination of a deep neural network architecture with transfer learning.
There is a also a lot of related work that focuses on creating large datasets such as \cite{chung2019conan} who create a large-scale, multilingual, expert based dataset of hate speech. 

What is less common in the research area of hate speech is analysis of relationships between different types of hate speech and the importance of specific keywords.

\textcolor{red}{That is why we we use smth to do smth etc. torej neki o teh modelih ki jih boma uporabljala. Pa neki o tem kere classe npr mama oz keri obstajajo pa kere boma midva raziskovala}

This paper is organized as follows:...
%------------------------------------------------

\section*{Data}

We use four publicly available datasets for our exploratory analysis. We combine datasets \cite{waseem2016you}, \cite{rizoiu2019transfer}, and \cite{jha2017does} into one large dataset (reffered to as Dataset SRB) as they include same categories of hate speech. We make labels \textit{sexism}, \textit{racism}, and \textit{both} from \cite{waseem2016you} and \cite{rizoiu2019transfer}. The third dataset (\cite{jha2017does}) that we use contains label \textit{hostile sexism}, where marked tweets are already included in the first two datasets under \textit{sexism}, and label \textit{benevolent sexism}, which we rename to \textit{benevolent}. We obtain a dataset with $6069$ samples that are labeled either $sexism$, $racism$, $both$, or $benevolent$.

The fourth dataset (reffered to as Dataset AHS) that we use \cite{founta2018large} has $4$ categories - \textit{abusive,
hateful, spam}. As this is the original dataset no additional merging is needed. We obtain a dataset with $13776$, with mentioned labels. Note that we exclude \textit{None} label from both datasets, as we do not need it for the analysis.

We show the distribution of individual categories from datasets SRB and AHS in Figures \ref{fig:distribution_tweets_dataset1} and \ref{fig:distribution_tweets_dataset2}, respectively. Note that the numbers of samples might not match the numbers in the original papers, due to the Twitter removing the tweets, making them unavailable for us to analyze.

We also provide an example for each label from both datasets. Some examples from the SRB dataset:


\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt]
\labelbox{\textit{Racism}} - "He can't be a server at our restaurant, that beard makes him look like a terrorist." Everyone laughs. \#fuckthanksgiving
\end{tcolorbox}


\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt]
\labelbox{\textit{Sexism}} - \#katieandnikki stop calling yourselves pretty and hot..you're not and saying it a million times doesn't make you either...STFU
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt]
\labelbox{\textit{Benevolent}} - It's "NEXT to every successful man, there's a woman"
\end{tcolorbox}

\noindent Examples from the AHS dataset:

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt]
\labelbox{\textit{Spam}} - RT @OnlyLookAtMino: [!!] \#WINNER trending \#1 on melon search
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt]
\labelbox{\textit{Abusive}} - You Worried About Somebody Bein Ugly... Bitch You Ugly...
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt]
\labelbox{\textit{Hateful}} - i hope leaders just kick retards that fake leave teams today
\end{tcolorbox}


\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{distribution_tweets_dataset1.png}
	\caption{\textbf{Distribution of tweets in SRB dataset.} This figure shows the distribution of hate speech categories in the SRB dataset. We can see that \textit{sexism} and \textit{benevolent} are well represented, whereas \textit{racism} and \textit{both} are far less frequent. Original set contains more tweets labeled \textit{racism}, but due to their removal we cannot obtain them.}
	\label{fig:distribution_tweets_dataset1}
\end{figure}

\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{distribution_tweets_dataset2.png}
	\caption{\textbf{Distribution of tweets in AHS dataset.} We see that the \textit{spam} is the most represented label in the dataset, which represents the majority of the dataset. This is followed by the \textit{abusive} tweets and there is the least \textit{hateful} tweets. We can see that categories in this dataset are well represented.}
	\label{fig:distribution_tweets_dataset2}
\end{figure}

\section*{Data preprocessing}
Before applying any methods we first preprocess all of our data. We separate a whole dataset AHS on three parts, one for each category (abusive, hateful, spam), each containing multiple documents - tweets belonging to this category. Similarly we separate a whole dataset SRB on three parts, one for each category (sexist, racist, benevolent). We remove retweet text RT, hyperlinks, hashtags, taggings, new lines, and zero length tweets. We further filter out tokens that not contain letters, e.g., raw punctuation.

\section*{Methodology}
\subsection*{TF-IDF}
We start off with a traditional method TF-IDF as we want to see the most relevant words for each category of offensive language. We show the results in Table \ref{tab:tf-idf}

\begin{table}[]
\begin{tabular}{ll}
category   & unigrams with highest tf-idf score \\
racism     & white, terror, coon                \\
sexism     & hot, hard, feminazi                \\
benevolent & nasty, sassy, classy               \\
abusive    & hoe, fake, love                    \\
hateful    & lgbt, religion, discrimination     \\
spam       & game, laptop, giveaway            
\end{tabular}
\end{table}
%------------------------------------------------

\section*{Discussion}

Use the Discussion section to objectively evaluate your work, do not just put praise on everything you did, be critical and exposes flaws and weaknesses of your solution. You can also explain what you would do differently if you would be able to start again and what upgrades could be done on the project in the future.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}