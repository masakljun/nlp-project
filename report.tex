%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{tcolorbox}
\usepackage{calc}
\usepackage{booktabs}


\newsavebox\mybox
\newenvironment{aquote}[1]
  {\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
  {\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}

\graphicspath{{fig/}}


\newtcbox{\labelbox}[1][red]{on line,
arc=0pt,outer arc=0pt,colback=#1!10!white,colframe=#1!50!black,
boxsep=0pt,left=1pt,right=1pt,top=1pt,bottom=0.8pt,
boxrule=0pt,bottomrule=0.6pt,toprule=0.6pt,width=5cm}

\newcommand\bm{0.2pt}


%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2021}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Offensive language exploratory analysis} 

% Authors (student competitors) and their info
\Authors{Maša Kljun, Matija Teršek}

% Advisors
\affiliation{\textit{Advisors: Slavko Žitnik}}

% Keywords
\Keywords{Hate speech, natural language processing, traditional methods, contextual and non-contextual embeddings, exploratory analysis, ...}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
In this paper we focus on the exploratory analysis of 21 different subgroups of hate speech. We use natural language processing techniques in order to find the underlying structure and connections/relations between the subgroups. We focus on data extracted from Twitter and online forums. First we use classic approaches, such as TF--IDF, BoW, and LDA, then we move on to more sophisticated methods, such as embeddings. We use both non-contextual embeddings, such as Word2Vec and GloVe, and contextual embeddings, such as BERT. We find out that \textcolor{blue}{TODO: Describe the findings for the last submission}.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
In the last few years social media grew exponentially and with it also the ability of people to express themselves online. Enabling people to write on different online platforms without even identifying themselves lead to a new era of freedom of speech. Despite this new medium for communication bringing many positive things, it also has its downside. Social media has become a place where heated discussions happen and often result in insults and hatred. It is an important task to recognize hate speech and to prevent it.

Hate speech is defined as \textit{abusive or threatening speech or writing that expresses prejudice against a particular group, especially on the basis of race, religion, or sexual orientation}\cite{hate_speech}. We can see that the definition is very vague. Having said that, the goal of our paper is to help distinguish different types of hate speech and find the specific keywords of its subgroups in order to explain its structure. This could help with its identification and classification. In this paper we focus on 21 subgroups of hate speech - \textit{abusive, hateful, spam, general hate speech, profane, offensive, cyberbullying, racism, sexism, vulgar, homophobic, slur, harassment, obscene, threat, discredit, insult, hostile, toxic, identity hate} and \textit{benevolent sexism}. The goal of this paper is to explore hate speech subgroups and understand the similarities and connections between them. 

There has been done a lot of research regarding the hate speech, however, these works are usually focused on the classification of hate speech. One of the first works include \cite{spertus1997smokey} who built the decision tree based classifier Smokey for abusive message recognition and classification. Some other works that focus mainly on classification include \cite{waseem2016you} who compare the classification accuracy of models trained on expert and amateur annotations, \cite{gamback2017using} who use convolutional neural networks for classification into four predefined categories, and \cite{martins2018hate} who use different natural language processing techniques for expanding datasets with emotional information for better classification. In the last years, especially deep learning models are often used for detection and classification of hate speech, such as \cite{rizoiu2019transfer} who propose a sophisticated method that is a combination of a deep neural network architecture with transfer learning.
There is a also a lot of related work that focuses on creating large datasets such as \cite{chung2019conan} who create a large-scale, multilingual, expert based dataset of hate speech. 

What is less common in the research area of hate speech is analysis of relationships between different types of hate speech and the importance of specific keywords. Some examples include \cite{xu2012learning}, who try to separate bullying from other social media posts and try to discover topic of bullying using topic modeling with Latent Dirichlet Allocation (LDA). \cite{calderon2020topic} model hate speech against immigrants on Twitter in Spain. They try to find underlying topic of hate speech using LDA, discovering features of different dimensions of hate speech, including foul language, humiliation, irony, etc. \cite{schmidt2017survey} conduct a survey about hate speech detection and describe key areas that have been explored, regarding the topic modeling, as well as sentiment analysis.

This paper is organized as follows: we present the datasets of tweets and comments, and describe data preprocessing in Section \ref{sec:data}, we perform the exploratory analysis by using many traditional and neural approaches in Section \ref{sec:metho}, and we show the final results and a scheme of hate speech in \textcolor{blue}{TODO: Complete this in the last submission}.

% --------------------------------------------------------------------------------------------------------------------------------

\section{Data}
\label{sec:data}

We use $7$ publicly available datasets for our exploratory analysis. We combine datasets \cite{waseem2016you}, \cite{rizoiu2019transfer}, and \cite{jha2017does} into one large dataset (referred to as Dataset SRB) as they include same categories of hate speech. We make labels \textit{sexism}, \textit{racism}, and \textit{both} from \cite{waseem2016you} and \cite{rizoiu2019transfer}. The third dataset (\cite{jha2017does}) that we use contains label \textit{hostile sexism}, where marked tweets are already included in the first two datasets under \textit{sexism}, and label \textit{benevolent sexism}, which we rename to \textit{benevolent}. We obtain a dataset with $6069$ samples that are labeled either $sexism$, $racism$, $both$, or $benevolent$.
The fourth dataset (referred to as Dataset AHS)\cite{founta2018large} that we use has $3$ categories - \textit{abusive,
hateful, spam}. As this is the original dataset no additional merging is needed. We obtain a dataset with $13776$ tweets with the mentioned labels. Note that we exclude \textit{None} label from both datasets, as we do not need it for the analysis.
We show the distribution of individual categories from datasets SRB and AHS in Figures \ref{fig:distribution_tweets_dataset1} and \ref{fig:distribution_tweets_dataset2}, respectively. Note that the numbers of samples might not match the numbers in the original papers, due to the Twitter removing the tweets, making them unavailable for us to analyze. We also provide an example for each label.

\begin{tcolorbox}[colback=black!8,width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Racism}} - "He can't be a server at our restaurant, that beard makes him look like a terrorist." Everyone laughs. \#fuckthanksgiving
\end{tcolorbox}

\begin{tcolorbox}[colback=black!8, width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Sexism}} - \#katieandnikki stop calling yourselves pretty and hot..you're not and saying it a million times doesn't make you either...STFU
\end{tcolorbox}

\begin{tcolorbox}[colback=black!8, width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Benevolent}} - It's "NEXT to every successful man, there's a woman"
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Spam}} - RT @OnlyLookAtMino: [!!] \#WINNER trending \#1 on melon search
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Abusive}} - You Worried About Somebody Bein Ugly... Bitch You Ugly...
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Hateful}} - i hope leaders just kick retards that fake leave teams today
\end{tcolorbox}

\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{distribution_tweets_dataset1.png}
	\caption{\textbf{Distribution of tweets in SRB dataset.} This figure shows the distribution of hate speech categories in the SRB dataset. We can see that \textit{sexism} and \textit{benevolent} are well represented, whereas \textit{racism} and \textit{both} are far less frequent. Original set contains more tweets labeled \textit{racism}, but due to their removal we cannot obtain them.}
	\label{fig:distribution_tweets_dataset1}
\end{figure}

\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{distribution_tweets_dataset2.png}
	\caption{\textbf{Distribution of tweets in AHS dataset.} We see that the \textit{spam} is the most represented label in the dataset, which represents the majority of the dataset. This is followed by the \textit{abusive} tweets and there is the least \textit{hateful} tweets. We can see that categories in this dataset are well represented.}
	\label{fig:distribution_tweets_dataset2}
\end{figure}

\noindent Additionally, we use the dataset of comments extracted from the League of Legends community \cite{bretschneider2016detecting}. We preprocess the dataset given in the SQL format to a more readable CSV form and keep only the posts that are annotated as harassment. We obtain $259$ examples of cyberbullying examples. The sixth dataset that we use was designed for the problem of the hate speech identification and classification, but we use the labels from the train and test set and merge them into one big dataset that we use for our analysis. It provides tags of \textit{hatespeech, profane}, and \textit{offensive}, so we refer to the dataset as HPO. It consists of 2549 tweets, distribution of which can be seen in Figure \ref{fig:distribution_tweets_hpo}. We again provide an example for each of the labels.

\begin{tcolorbox}[colback=black!8, width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Cyberbullying}} - plot twist she's a fggt
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Hatespeech}} - Johnson you liar. You don't give a flying one for the Irish
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Offensive}} - \#FuckTrump And retired porn star Melania too.
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Profane}} - Fuck Trump and anybody who voted for that Lyin POS!  \#FuckTrump
\end{tcolorbox}

\begin{figure}[ht]\centering
	\includegraphics[width=\linewidth]{distribution_tweets_hpo.png}
	\caption{\textbf{Distribution of tweets in HPO dataset.} The most used label is \textit{hatespeech}. It is followed by \textit{profane} and then \textit{offensive}, which have a similar number of tweets.}
	\label{fig:distribution_tweets_hpo}
\end{figure}

\noindent We also use the dataset of Wikipedia comments \cite{dixon2017ex}, that are marked as either \textit{toxic}, \textit{sever toxic}, \textit{obscene}, \textit{identity hate}, \textit{threat}, and \textit{insult}. We merge the first two categories into \textit{toxic}. It is important to note that each comment in this dataset might have multiple labels, so the results for those tags might be similar. Original dataset contains $159571$ tweets, $16225$ of which are labeled. We show the distribution of the labels in Figure \ref{fig:distribution_wiki_dataset}. We denote this dataset as TOITI in the future text.

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Threat}} - SHUT UP, YOU FAT POOP, OR I WILL KICK YOUR ASS!!!
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Obscene}} - you are a stupid fuck and your mother's cunt stinks
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Insult}} - Fuck you, block me, you faggot pussy!
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Toxic}} - What a motherfucking piece of crap those fuckheads for blocking us!
\end{tcolorbox}

\begin{tcolorbox}[width=0.9\linewidth, center,arc=8pt,sharp corners=downhill, boxrule=0.3pt, left=\bm, top=\bm, right=\bm, bottom=\bm, fontupper=\small]
\labelbox{\textit{Identity}} - A pair of jew-hating weiner nazi schmucks.
\end{tcolorbox}


\begin{figure}
	\includegraphics[width=\linewidth]{distribution_wiki_dataset.png}
	\caption{\textbf{Distribution of tweets in TOITI}. We see that most of the comments are labeled as \textit{toxic}. Around half of them are \textit{obscene} and around half are also labeled as \textit{insult}. \textit{Identity hate} and \textit{threat} are far more uncommon in this dataset.}
	\label{fig:distribution_wiki_dataset}
\end{figure}

\subsection{Data preprocessing}
Before applying any methods we first preprocess all of our data. We remove retweet text RT, hyperlinks, hashtags, taggings, new lines, and zero length tweets. We further filter out tokens that do not contain letters, e.g., raw punctuation.

% --------------------------------------------------------------------------------------------------------------------------------
\section{Methodology}
\label{sec:metho}
We start the analysis with more traditional approaches, and continue with neural approaches.

% --------------------------------------------------------------
\subsection*{LDA}
We use Latent Dirchilet Allocation (LDA) in combination with Bag-of-Words (BoW) and TF--IDF in hopes of finding obvious topics from all the provided comments / tweets. We try to determine 15 different topics, which is the same as the number of labels we have in our datasets. Results using BoW and TF--IDF are similar, however, we cannot clearly distinguish between the topics and connect obtained topics to the existing labels, aside from one topic, which is related to sexism. Top 5 most related words are: \textit{penis, rape, image, live, vagina}.

% --------------------------------------------------------------
\subsection*{TF-IDF}
We continue with the analysis of datasets with a traditional method TF--IDF as we want to see the most relevant words for each category of offensive language that we have in the dataset. We show the results in Table \ref{tab:tf-idf}. We can see that some of the categories have similar unigrams that achieved the highest TF--IDF score. An example of categories with the same highest scored unigrams are \textit{insult} and \textit{obscene}. This makes it harder to differentiate between the categories. It is important to note, that such examples might also occur due to subjective labeling in the provided datasets, as well as people not clearly differentiating between these categories. Most datasets are not labeled by experts, but with the help of platforms such as FigureEight or Amazon Mechanical Turk. From the results in Table \ref{tab:tf-idf}, we could assume that most people perceive categories such as \textit{insult} and \textit{obscene} or \textit{threat} and \textit{toxic} similarly. On the other hand, categories such as \textit{spam} or \textit{cyberbullying} are clearly differentiable from other categories. We can also see a lot of categories including Trump related words (\textit{hatespeech, profane}, and \textit{offensive}). Those categories are taken from the same dataset, and we can see that such labels will contain words that are related. So the words connected to those labels might also be connected to some bigger topic, which depends on the annotator's choice from where to extract the tweets / comments.

\begin{table}[htb]
\scriptsize
\centering
\begin{tabular}{l|l}
\toprule
\textbf{category}   & \textbf{unigrams with highest TF--IDF score} \\ \midrule
racism     & peopl, white, terror, man, look                \\ \hline
sexism     & feminazi, women, think, sexist, notsexist                \\ \hline
benevolent & women, classi, sassi, nasti, gonna            \\ \hline
abusive    & know, stupid, shit, like, idiot  \\ \hline
hateful    & peopl, trump, nigga, like, idiot    \\ \hline
spam       & giveaway, game, enter, work, home            \\ \hline
cyberbullying       & one, guy, good, gone, go             \\ \hline
hatespeech      & world, trumpisatraitor, trump, shameonicc, peopl             \\ \hline
identity hate       & fuck, shit, littl, like, one             \\ \hline
insult       & delet, go, ass, stupid, bitch        \\ \hline
obscene       & delet, go, stupid, bitch, ass             \\ \hline
offensive       & trumpisatraitor, like, douchebag, fucktrump, get            \\ \hline
profane       & trump, shit, say, resist, peopl             \\ \hline
threat       & fuck, get, die, want, find           \\ \hline
toxic       & fuck, get, bitch, want, block             \\ \bottomrule
\end{tabular}
\caption{Table shows 5 highest scoring unigrams for each label we investigate. We choose the parameters, which we believe provide us the most meaningful unigrams, so we consider words that appear in at least $5\%$ and less than $60\%$ of the documents.}
\label{tab:tf-idf}
\end{table}
% --------------------------------------------------------------
\subsection*{Non-contextual word embeddings}
For each of the category labels we try to find the $30$ most similar words and use their embeddings to infer the similarities and differences between the subgroups. For this task we use pre-fitted Word2Vec (\cite{mikolov2013efficient}, \cite{mikolov2013distributed}), GloVe \cite{pennington2014glove}, and FastText (\cite{bojanowski2016enriching}). We visualize the results with the help of t-SNE. Because of this we cannot interpret distances between the labels from the visualization. However, we can still infer that the labels that are intertwined are more similar than those that are nicely separable from one another. %We show the results in \ref{fig:embedding_words}. From both plots we can see that words (and its neighbors) like bitch, ass, nigga, shit, fuck, idiot, stupid, and docuhebag are relatively closely together. This could indicate a relation between \textit{abusive, hateful, insult, obscene, identity hate}, and \textit{toxic}. We can also see that classy, nasty, sassy, feminazi, and sexist are closely related - sometimes some words are more related in Word2Vec than in Glove and vice versa. From this we can see a relation between \textit{sexism} and \textit{benevolent sexism}, which is expected as they are correlated. Relationships between certain words can also vastly differ in Word2Vec and Glove. For example giveaway and game are relatively close in Word2Vec, but are further apart in Glove. Similarly, terror and white (both common unigrams of \textit{racism} label) are far away in Word2Vec, but close in Glove. However, terror stands out from other words in both embeddings, which might indicate that racism is at least in some way different to other labels. Similarly, words from \textit{spam} are usually more separated from words of other labels, also indicating another more clearly distinguishable group. Trump is relatively close to a lot of words in Word2Vec - women, world, peopl, sexist, die, little, guy. This could imply that general \textit{hatespeech} and \textit{hateful} are connected to \textit{sexism, identity hate, threat} and also \textit{cyberbullying}. 

%\begin{figure*}[htb]\centering
%	\includegraphics[width=0.495\linewidth]{SimilarWords - word2vec - t-SNE.png}
%	\includegraphics[width=0.495\linewidth]{SimilarWords - Glove - t-SNE.png}
%	\caption{\textbf{Word2Vec and Glove similar words.} Left figure shows Word2Vec embeddings of neighboring words of top unigrams from Table \ref{tab:tf-idf} and the right figure shows Glove embeddings.}
%	\label{fig:embedding_words}
%\end{figure*}

We show the results in Figure~\ref{fig:embedding_labels}. We can see that \textit{homophobic} and \textit{racist} appear very intertwined in Word2Vec and GloVe embeddings, meaning that they cannot be separated, thus indicating a strong relation. On the other hand, in both of these embeddings \textit{spam}, \textit{toxic}, and \textit{discredit} are well separated from other groups and are clearly distinguishable from others. We can also see that \textit{abusive} is entangled with \textit{benevolent} in GloVe representation, however, in results obtained from Word2Vec  \textit{benevolent} is nicely separable from other labels. So it is difficult to conclude that \textit{benevolent} is a label that is different enough from other labels. FastText also nicely separates \textit{toxic} and \textit{benevolent} from other labels, but is unable to separate \textit{vulgar}, \textit{profane} and \textit{obscene}, and \textit{insult}. From all three models combined, we can conclude that the only label that can be always well distinguished from the others is \textit{toxic}, and that \textit{vulgar}, \textit{profane}, \textit{obscene}, and \textit{insult} are labels that cannot be nicely separated. We also conclude that \textit{spam} is a nicely separable category. Note that in some models we omit labels that are not in a vocabulary.


\begin{figure}[!t]
	\centering
	\includegraphics[width=\linewidth]{SimilarWords - word2vec - t-SNE_1.png}
	\includegraphics[width=\linewidth]{SimilarWords - Glove - t-SNE_1.png}
	\includegraphics[width=\linewidth]{SimilarWords - FastText - t-SNE_1.png}
	\caption{\textbf{Word2Vec, Glove, and FastText similar labels.} Figure shows Word2Vec (1st row), Glove (2nd row), and FastText (3rd row) embeddings of neighboring words of labels we analyze. Note that we omit hate speech subgroups that are not in the vocabulary.}
	\label{fig:embedding_labels}
\end{figure}

By now we provide some relations and decide to further investigate the connections between the related labels using word analogy. We try to find hyponyms and hypernyms, which we do with the help of the following setting:
\begin{verbatim}
father : son = our_label : x 	(hyponyms)
animal : cat = our_label : x 	(hyponyms)
son : father = our_label : x 	(hypernyms)
cat : animal = our_label : x 	(hypernyms)
\end{verbatim},
where \texttt{our\_label} is one of the analyzed labels and \texttt{x} is the word found by Word2Vec or Glove. 

Unfortunately, the relationships are not clear and uniquely defined. An example is \textit{racism} is to \textit{sexism} what is \texttt{son} to \texttt{father} with $\approx 64.6\%$ probability, but \textit{sexism} is to \textit{racism} what is \texttt{son} to \texttt{father} with $\approx 64.8\%$ probability. We can once again see that the two labels are related, but the precise relationship cannot be inferred. Using \texttt{brother} and \texttt{sister} the probability is lower. This could indicate that it is impossible to find a specific hypernym and that we can only conclude that the labels are more closely related to each other, as they are each in some way hypernym and hyponym of each other. Similarly, \textit{racism} and \textit{sexism} are connected to \textit{homophobia} and \textit{slur}. Another group that we find, but also cannot clearly define the inner relations contains \textit{vulgar}, \textit{profane}, and \textit{obscene}.

As mentioned, the distances between the inspected labels cannot be determined from our chosen visualization. That is why we approach this problem with clustering. We use $k$-means and hierarchical clustering in hopes of finding meaningful clusters that could help us understand the relationships between the subgroups of the hate speech better. We determine the $k$ in $k$-means by using the silhouette score. Note that we choose the $k$ of the second peak of the score, as we want to form more diverse and meaningful clusters than just $2$ big subgroups as the silhouette score suggests. See the example output of the silhouette score in Figure \ref{fig:silh}.

\begin{figure}[htb]\centering
	\includegraphics[width=\linewidth]{silh.png}
	\caption{\textbf{Silhouette score.} Example of silhouette scores for different numbers of clusters. We use the second peak ($k = 6$) instead of first ($k = 2$), as we want to get more clusters.}
	\label{fig:silh}
\end{figure}

From the top 30 similar words for each label, we compute an average vector and we obtain one such vector for each label. We compute the cosine similarity matrix between the vectors $simcos$ and compute the distance matrix as $d = 1 - simcos$, which we then use for the clustering. In Table \ref{tab:clustering_labels_word2vec} we show the obtained clusters and in Figure \ref{fig:hier} we show the results of hierarchical clustering of Word2Vec embeddings.

From these two clustering results we can infer that \textit{insult} and \textit{obscene} are two similar subgroups of hate speech as they both appear in the same cluster in+ $k$-means clustering and we can see that they are closely together in hierarchical clustering. They are also very similar according to the results from TF--IDF as seen before. \textit{Benevolent sexism} is also close. We can see that \textit{cyberbullying} and \textit{spam} are clustered together in both clusterings and that \textit{threat} and \textit{toxic} are also very similar. From the results of hierarchical clustering we can see that \textit{offensive} is also close to them.

\begin{figure}[htb]\centering
	\includegraphics[width=\linewidth]{hier.png}
	\caption{\textbf{Hierarchical clustering of average Word2Vec embeddings of labels' 30 nearest words.}}
	\label{fig:hier}
\end{figure}

Comparing the hierarchical clustering results of Glove and FastText embeddings to Word2Vec embeddings, we can see that we always get almost the same two main clusters as those in Figure \ref{fig:hier}, so we do not show figures with those results. 

Looking at $k$-means clustering of Word2Vec and GloVe embeddings we see that pairs of \textit{abusive}, \textit{vulgar}, \textit{racist}, \textit{homophobic}, \textit{profane}, \textit{slur}, \textit{obscene}, \textit{hateful} and \textit{insult}, and \textit{discredit} and \textit{hostile} always appear in the same two clusters, so we can conclude that they are related. We do not include the results of FastText $k$-means clustering, as its silhouette score is $\le 0.30$ for all possible $k$, whereas in the first two, the score is often $> 0.30$. 

We try to apply this same approach to the words with highest TF--IDF scores from each subgroup, however, the obtained clusters provide no useful understanding, so we omit those results.

\begin{table}[htb]
\begin{tabular}{c|l}
\toprule
\textbf{cluster} & \textbf{components}                                                                                                                                        \\ \midrule
1                & offensive                                                                                                                                                  \\ \hline
2                & \begin{tabular}[c]{@{}l@{}}abusive, vulgar, racist, homophobic, profane, \\ slur, harassment, obscene, hateful, insult,\\ sexism, hate speech\end{tabular} \\ \hline
3                & discredit, hostile, benevolent                                                                                                                             \\ \hline
4                & cyberbullying, spam                                                                                                                                        \\ \hline
5                & threat, toxic                                                                                                                                              \\ \bottomrule
\end{tabular}
\caption{\textbf{$K$-means clustering of average Word2Vec embeddings of labels' 30 nearest words.} Table shows five clusters obtained with 5-means clustering. We determine $k = 5$ using silhouette score.}
	\label{tab:clustering_labels_word2vec}
\end{table}


\begin{figure}[htb]\centering
	\includegraphics[width=\linewidth]{bert_tsne.png}
	\caption{\textbf{BERT embeddings.} T-SNE visualization of BERT embeddings of labels.}
	\label{fig:berttsne}
\end{figure}
\subsection*{Contextual word embeddings}
We move on to contextual embeddings and we focus on BERT. We use the pretrained BERT base cased model \cite{dbpl} and convert tweets and comments from our dataset to BERT embeddings. We first append them \texttt{ - This is <label>} and compute the embeddings. From obtained embeddings of each vector, we compute an average representation from the vectors that belong to the tokens of the label. We average the obtained representation of each label and use cosine similarity to compute the similarity between those label representations. We show the obtained similarity matrix in Figure \ref{fig:bertsim1}. We can see high similarities between most of the subgroups of hate speech. The one that differs the most from the other groups is \textit{cyberbullying}. We can also see that \textit{profane} is slightly less similar to \textit{identity}, \textit{insult}, \textit{threat}, and \textit{toxic}, however, the similarity score is still between $0.87$ and $0.89$. For all other combinations the similarity score is $\ge 0.90$. We also visualize the embeddings with the help of t-SNE in Figure \ref{fig:berttsne} and we show the labels on the mean points of each subgroup. We can see that all subgroups are tightly connected and it is hard to distinguish between them. However, we can see that \textit{cyberbullying} is a little bit more compact and not as dispersed as others, which might be a reason behind slightly different similarity scores. It is also interesting that some labels, although being dispersed, have some small clusters which stand out and might indicate special subgroups within those subgroups of hate speech. Example of such subgroup is \textit{benevolent sexism}.

\begin{figure}[htb]\centering
	\includegraphics[width=\linewidth]{bert_sim1.png}
	\caption{\textbf{Cosine similarities between average representations of label embeddings.} }
	\label{fig:bertsim1}
\end{figure}


Using sentence transformers \cite{reimers-2019-sentence-bert} we obtain embeddings for each tweet / comment without the \texttt{This is <label>} appendix. For each label we try to find the keywords that describe subgroups of hate speech the most. We do this by first embedding the tweets / comments using BERT and separately embedding the sub-phrases from documents. For each document we then try to find the most similar sub-phrases with the help of embeddings and cosine similarity. For each label we provide three of the most common keywords obtained from the documents. We show the keywords in Table \ref{tab:bertkey}.

\begin{table}[]
\scriptsize
\centering
\begin{tabular}{l|l}
\toprule
\textbf{category} & \textbf{BERT keywords}                                                                                                                                \\ 
\midrule
racism            & \begin{tabular}[c]{@{}l@{}}racistreading, terroristreligion, terroristnationpakistan, \\ faithlessfaggotboy, racisttrying\end{tabular}                \\ \hline
sexism            & \begin{tabular}[c]{@{}l@{}}godlesswomen, sexlifeless, bitchmattythewhite,\\  onlyallwomen, bitchesvagina\end{tabular}                                 \\ \hline
benevolent        & \begin{tabular}[c]{@{}l@{}}sexyolderwomen, girlsbeautiful, bitchmattythewhite,\\  womenattractmenattracted, happywomenday\end{tabular}                \\ \hline
abusive*          & \begin{tabular}[c]{@{}l@{}}misogynistic, hatemyowngender, hatefultrump,\\  selfishmotherfuckers, faithlessfaggotboy\end{tabular}                      \\ \hline
hateful*          & \begin{tabular}[c]{@{}l@{}}misogynistic, hatemyowngender, selfishmotherfuckers, \\ terroristhasreligion, faithlessfaggotboy\end{tabular}              \\ \hline
spam              & \begin{tabular}[c]{@{}l@{}}100million, enjoyed, enjoying, \\ fridaymotiovation, saturdaynightonline\end{tabular}                                      \\ \hline
cyberbullying*    & \begin{tabular}[c]{@{}l@{}}whoremonger, selfishmotherfuckers, faithlessfaggotboy, \\ bitchmattythewhite, boycottfoxnews\end{tabular}                  \\ \hline
hatespeech*       & \begin{tabular}[c]{@{}l@{}}doctors\_against\_assualt, trumpobstructedjustice, \\ trumpfascism, terroristnationpakistan, doctorsfightback\end{tabular} \\ \hline
identity hate     & \begin{tabular}[c]{@{}l@{}}whoremonger, gaywad, nazisnotwelcome, \\ racisttrying, racistreading\end{tabular}                                          \\ \hline
insult*           & \begin{tabular}[c]{@{}l@{}}whoremonger, selfishmotherfuckers, boycottfoxnews, \\ killyourself, cocksuckerfuck\end{tabular}                            \\ \hline
obscene*          & \begin{tabular}[c]{@{}l@{}}boycottfoxnews, whoremonger, killyourself, \\ selfishmotherfuckers, cocksuckerfuck\end{tabular}                            \\ \hline
offensive*        & \begin{tabular}[c]{@{}l@{}}trumpobstructedjustice, trumpfascism, \\ selfishmotherfuckers, murdermystery, terroristhasreligion\end{tabular}            \\ \hline
profane*          & \begin{tabular}[c]{@{}l@{}}trumpfascism, trumpobstructedjustice, faithlessfaggotboy, \\ selfishmotherfuckers, doctors\_against\_assualt\end{tabular}  \\ \hline
threat            & \begin{tabular}[c]{@{}l@{}}killyourself, murdering, whoremonger, \\ hatemongers, murdermystery\end{tabular}                                           \\ \hline
toxic*            & \begin{tabular}[c]{@{}l@{}}boycottfoxnews, selfishmotherfuckers, killyourself, \\ whoremonger, nazisnotwelcome\end{tabular}   

\\
\bottomrule                       
\end{tabular}
\caption{\textbf{BERT keywords.} Table shows 5 most important keywords for each hate speech subgroup found with BERT. Note: categories that have an $\*$ contain also 2 keywords (dumbdonnythedraftdodgingdotart and worstsecretaryofstateinushistory) but we omit them in order to obtain clearer represenatation of categories.}
\label{tab:bertkey}
\end{table}

%\subsection{Universal Sentence Encoder}
%\textcolor{blue}{In the last submission we will also include the findings and results of USE that are already present in our repository.}

 \section*{Analysis}
Considering all the results and findings from above, we can now provide the following inference. We can define the following "islands", which means that the subgroups in one island are connected to each other more than to subgroups from other islands. Those subgroups, that are an island by themselves, are the subgroups that can be nicely separated from all other subgroups. 
\begin{enumerate}
  \item \textit{sexism}, \textit{racism}, \textit{homophobic}, and \textit{slur} 
  \item \textit{obscene}, \textit{insult}, \textit{discredit}
  \item \textit{vulgar}, \textit{profane}
  \item \textit{hate speech}
  \item \textit{benevolent}
  \item \textit{toxic}
  \item \textit{spam}
  \item \textit{cyberbullying}
  \item \textit{threat}, \textit{hostile}, \textit{offensive}
\end{enumerate}
Note: the above list includes 17 out of 21 subgroups that we analyse. The reason behind is that \textit{abusive} is a subgroup that can be connected with islands 1,2, or 5. Same holds for \textit{hateful}, which is connected to the same islands, however, it is never connected to \textit{abusive}. Another subgroup that is sometimes connected to island 8 is \textit{harassment}. \textcolor{red}{identity?}


\section*{Discussion}

\textcolor{blue}{In the last submission we are going to discuss the findings and provide the schema of hate speech.}

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}