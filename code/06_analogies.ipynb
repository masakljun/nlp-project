{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fa804b",
   "metadata": {},
   "source": [
    "# Analogies of Word2Vec and GloVe\n",
    "\n",
    "In this report we analyze the Word2Vec and GloVe analogies.\n",
    "\n",
    "NOTE: If you installed the environment from *environment.yml* you need to additionally install *gensim* library before running this code. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlled-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv = gensim.downloader.load('word2vec-google-news-300')\n",
    "#model_g = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-province",
   "metadata": {},
   "source": [
    "## How to use analogies?\n",
    "\n",
    "Here we provide an example of how we use the analogies. You can get the results we get by changing the words in positive list.\n",
    "\n",
    "#### Find pairs\n",
    "Paris is to France as Berlin is to ?\n",
    "\n",
    "Paris : France :: Berin : ?\n",
    "\n",
    "France - Paris + Berlin = ?\n",
    "\n",
    "? = Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-damages",
   "metadata": {},
   "source": [
    "sims = model_wv.most_similar(positive=['France', 'Berlin'], negative=['Paris'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-lawsuit",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interested-influence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('institutionalized_racism', 0.709613561630249),\n",
       " ('homophobia', 0.7021568417549133),\n",
       " ('Racism', 0.6871979236602783),\n",
       " ('racial_discrimination', 0.6775422692298889),\n",
       " ('bigotry', 0.6737836599349976),\n",
       " ('racial_prejudice', 0.6732276082038879),\n",
       " ('racial_intolerance', 0.6541361808776855),\n",
       " ('racist', 0.6478180289268494),\n",
       " ('racial_bigotry', 0.6457424759864807),\n",
       " ('blatant_racism', 0.6429591178894043)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims1 = model_wv.most_similar(positive=['father', 'racism'], negative=['son'])\n",
    "sims1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valuable-repair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('racism', 0.7008050680160522), ('homophobic', 0.6619293689727783), ('bigotry', 0.621934175491333), ('homophobic_attitudes', 0.6141319274902344), ('Homophobia', 0.6102346777915955), ('homosexuality', 0.6007557511329651), ('gay_bashing', 0.5969531536102295), ('gay', 0.5882768034934998), ('racial_intolerance', 0.5835667252540588), ('sexism', 0.5829565525054932)]\n"
     ]
    }
   ],
   "source": [
    "sims1 = model_wv.most_similar(positive=['father', 'homophobia'], negative=['son'])\n",
    "print(sims1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "above-reading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('defensive', 0.5628560781478882),\n",
       " ('Offensive', 0.5497417449951172),\n",
       " ('offense', 0.5321450233459473),\n",
       " ('Dave_Borbely', 0.4998655915260315),\n",
       " ('guard_RJ_Mattes', 0.4856272041797638),\n",
       " ('coach_Bob_Palcic', 0.48259395360946655),\n",
       " ('coach_Jimmy_Heggins', 0.47518688440322876),\n",
       " ('offensively', 0.47009724378585815),\n",
       " ('coach_Dan_Cozzetto', 0.4652729034423828),\n",
       " ('coach_Dave_Magazu', 0.46430879831314087)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model_wv.most_similar(positive=['brother', 'offensive'], negative=['sister'])\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "resistant-mobility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('defensive', 0.6267510652542114),\n",
       " ('Offensive', 0.6130504608154297),\n",
       " ('coach_Bob_Palcic', 0.5567696690559387),\n",
       " ('offensively', 0.5524481534957886),\n",
       " ('offense', 0.5397348403930664),\n",
       " ('Dave_Borbely', 0.5268849730491638),\n",
       " ('guard_RJ_Mattes', 0.525318443775177),\n",
       " ('promoted_Pete_Metzelaars', 0.5250058174133301),\n",
       " ('coach_Jimmy_Heggins', 0.523449182510376),\n",
       " ('coach_George_Yarno', 0.5208775997161865)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model_wv.most_similar(positive=['father', 'offensive'], negative=['son'])\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "junior-detection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Offensive', 0.6361225843429565),\n",
       " ('defensive', 0.6109646558761597),\n",
       " ('offense', 0.5482767820358276),\n",
       " ('offensively', 0.5432956218719482),\n",
       " ('guard_RJ_Mattes', 0.5229091644287109),\n",
       " ('Offensively', 0.5038000345230103),\n",
       " ('coach_Bob_Palcic', 0.5025234818458557),\n",
       " ('Pashtun_Zarghun_district', 0.5010451078414917),\n",
       " ('coach_Jimmy_Heggins', 0.49560749530792236),\n",
       " ('playmakers', 0.4946232736110687)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model_wv.most_similar(positive=['son', 'offensive'], negative=['father'])\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-order",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wanted-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ruled-official",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('slurs', 0.5391111969947815),\n",
       " ('homophobic', 0.44801968336105347),\n",
       " ('derogatory', 0.44130533933639526),\n",
       " ('disparaging', 0.41294434666633606),\n",
       " ('epithets', 0.40569788217544556),\n",
       " ('uttered', 0.40462562441825867),\n",
       " ('racist', 0.4021945595741272),\n",
       " ('nigger', 0.39872634410858154),\n",
       " ('remark', 0.3884381055831909),\n",
       " ('insult', 0.37972599267959595)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims2 = model_g.most_similar(positive=['brother', 'slur'], negative=['sister'])\n",
    "sims2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "controlled-sodium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('slurs', 0.6233149170875549),\n",
       " ('derogatory', 0.5212036371231079),\n",
       " ('homophobic', 0.5057780146598816),\n",
       " ('epithets', 0.5045928955078125),\n",
       " ('uttered', 0.4736688435077667),\n",
       " ('disparaging', 0.4691711962223053),\n",
       " ('profanities', 0.4635857939720154),\n",
       " ('obscenities', 0.44784191250801086),\n",
       " ('nigger', 0.44120684266090393),\n",
       " ('remark', 0.4186045229434967)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims2 = model_g.most_similar(positive=['son', 'slur'], negative=['father'])\n",
    "sims2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "advisory-certificate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('slurs', 0.6650111675262451),\n",
       " ('derogatory', 0.5520740151405334),\n",
       " ('disparaging', 0.5134429931640625),\n",
       " ('homophobic', 0.5107932090759277),\n",
       " ('racist', 0.5039888620376587),\n",
       " ('epithets', 0.494453489780426),\n",
       " ('racial', 0.48461055755615234),\n",
       " ('stereotyping', 0.48268580436706543),\n",
       " ('sexist', 0.46767958998680115),\n",
       " ('semitic', 0.46574753522872925)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims2 = model_g.most_similar(positive=['father', 'slur'], negative=['son'])\n",
    "sims2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-albert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c23b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test1] *",
   "language": "python",
   "name": "conda-env-test1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
